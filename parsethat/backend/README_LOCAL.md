# How to run the local model

In order to run the model, you need several libraries. You can easily install them using `pip`.

```
pip install transformers jsonformer bitsandbytes
```
To run the model, simply run the file 'run_local.py', instead of the regular 'run.py'. It creates a different instance with the different endpoint `/api/parseit_local`.

The model loads on (device) memory on runtime (more precisely, when LLMAgent is created). 
The inference is executed once the PDF parsing phase ends. The structure is mostly the same with the OpenAI version, but the Pydantics validation part may not work as I broke my libraries while trying some stuff :p. A sample output generated by the model is included in the project directory as `llama.json`, you can check it to further validate.

Neither the quality of the output nor the execution time is not better than compared with the paid ChatGPT model, but if one has the capacity to run more capable models, it can be comparable. Different from yesterday, we can actually get a correctly structured output, which shows the feasibility of the open source models.
